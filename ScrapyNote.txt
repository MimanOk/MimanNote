-> 初始化项目：scrapy startproject project_name
    - scrapy genspider spider_name(不可与项目名相同) allow_spider_domain_name(允许爬的域名)

-> 使用不同模板生成spider
	- scrpay genspider -t crawl(还可以是其他的，默认base) spider_name domain

-> 用户默认配置：custom_settings = {
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en',
            'User-Agent': generate_user_agent()
        }
    }

-> 爬取数据写入文件：scrapy crawl spider.py -o xxx.(json or xml or csv)

-> 拼接下一页url：next_url = response.urljoin("body_url")    -暂时用不到

-> 调试网站：scrapy shell -s USER_AGENT="" url

-> 生成未被渲染的html：scrapy view url

-> xsrf是服务器端防止攻击的一种握手方式，是服务器端生成的一段随机码

-> 自定义爬取url格式：
    - rules = {
        # 提取匹配正则表达式'/group?f=index_group'链接，但是不能匹配'index.php'
        # 并且会递归爬取，如果没有定义callback，默认follow=True
        Rule(LinkExtractor(allow=('/group?f=index_group', ), deny=(index\.php, ))),
        # 提取匹配'/article/\d+/\d+.html'的链接，并且用parse_item来解析下载的内容，不递归
        # Rule(LinkExtractor(allow='/article/\d+/\d+\.html, '), callback='parse_item'),
    }

-> 下载管道
    - def open_spider(self, ...):
    - def close_spider(self, ...):
    - def process_item(self, item, spider)
	
# 该框架中的各种请求 #
-> 由于scrapy基于twisted的异步框架，可以同时处理多个请求，故每个方法返回的
	- 请求要么是生成器形式， 要么是列表形式
from scrapy import Request
from scrapy import FormRequest
-> get请求
	meta = {
		'proxy': proxy,
		'dont_retry': True,
		'download_timeout': 10,
	}
	return [Request(url=url, headers=headers, meta=meta, callback=func)]
	或:
	yield Request(url=url, headers=headers, meta=meta, callback=func)
	
-> post请求
	formdata = {
		'_xsrf': xsrf,
		'other': other,
	}
	return [FormRequest(url=url, formdata=formdata, headers=headers, callback=func)]
	或：
	yield FormRequest(url=url, formdata=formdata, headers=headers, callback=func)
	
# end #

# 在items中做预处理 #
-> 在scrapy.Field()中添加预处理函数：
    - 1. from scrapy.loader.processors import MapCompose, TakeFirst, Join
    scrapy.Field(input_processor=MapCompose(
		input_processor=TakeFirst()  # 取第一个
		output_processor = Join(",")  # 用","拼接))
    - 2. 提取item中每个field中的第一个数据（原本数据可能是列表形式）
    from scrapy.loader import ItemLoader
    class ArticleItemLoader(ItemLoader):
	""" 重载ItemLoader """
        deafault_out_processor = TakeFirst()
    再将Spider中的从items中导入的...ItemLoader()改为自定义的ArticleItemLoader()
    - 3. 重点！！！
    在配置2中的deafault_out_processor后，会将列表形式的图片url提取成字符串，在后续
    保存过程中会抛异常，所以需要image_urls = scrapy.Field(output_processor=MapCompose(lambda x: x))
    覆盖deafault_out_processor方法

# end #

-> Spider中要自己导入Request、FormRequest等模块

# urllib #
-> .parse方法可以拼接url：parse.urljoin(response.url, /body_url)

# end #

# 图片下载 #
-> 自带图片下载管道：在piplines中自定义类需要继承ImagePipeline父类
    - settings的ITEM_PIPLINES中配置：'scrapy.piplines.images.ImagesPipline': 301,
    - settings中直接配置image下载链接字段：IMAGES_URLS_FIELD = "front_image_url"
    - settings中直接配置图片保存路径：
    path = os.mkdir(os.path.join(os.path.dirname(os.path.abspath(__file__)), "images"))
    os.path.join(os.path.dirname(path, "images")
    IMAGES_STORE = os.path.join(path, "images")
    - items中front_image_url = scrapy.Field()
    - spider中给item的front_image_url键赋值时注意将爬到的front_image_url变成列表

    - 过滤图片，settings中直接配置
    - IMAGES_MIN_HEIGHT = size
    - IMAGES_MIN_WIDTH = size

    - 重载类方法1：get_media_requests(self, item, info)：Pipline将从item中获取图片的urls并下载，
    并返回一个Requests对象，这些请求对象将被Pipeline处理，当完成下载后，结果将发送到
    item_completed方法。这些结果为一个二元组的list（success，image_info_or_failture）。
    1. success：boolean值，true表示成功下载。
    2. image_info_or_failture：如果success等于True，image_info_or_failture词典包含以下键值对，失败侧包含一些出错信息。
    url：原始URl
    path：本地存储路径
    checksum：校验码
    示例：
    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            self.default_headers['referer'] = image_url
            yield Request(image_url, headers=self.default_headers)

    - 重载类方法2：item_completed(self, results, items, info)
    示例：
    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
            if not image_paths:
                raise DropItem("Item contains no images")
            item['image_paths'] = image_paths
            return item
    - 报错处理：PIL -->pip install pillow

# end #

-> 在pycharm中调试scrapy爬虫
    - from scrapy.cmdline import execute
    - import sys, os
    - sys.path.append(os.path.dirname(os.path.abspath(__file__)))  # 配置路径
    - execute(["scrapy", "crawl", "爬虫名"])  # 运行爬虫

-> 运行爬虫
    - scrapy crawl 爬虫名
    - 报错：No module named "win32api"解决：pip install -i 临时镜像地址（配置了永久的就不需要）pypiwin32

-> url拼接：response.urljoin(/body_url)

-> response.meta.get("key", "")：为避免报错，在meta中取值时最好用get，第二个参数为默认为空

-> 将数据以json格式保存至文件
    - piplines中
    class JsonWithEncodingPipeline(object):
        def __init__(self):  # 打开文件可以用codecs包来打开文件，可以避免很多有关编码问题
            self.file = codecs.open(r'file_path.json', 'w', encoding='utf-8')
        def process_item(self, item, spider):
            lines = json.dumps(dict(item), ensure_ascii=False) + '\n'
            self.file.write(lines)
            return item
        def spider_closed(self, spider):
            self.file.close()

    - scrapy自带将item以各种格式写入文件（与上面方法不同之处是写入后的json文件以b"["开头，以b"]"结尾）
    首先：from scrapy.exports import JsonItemExporter
    class JsonExporterPipleline(object):
        def __init__(self):
            self.file = open(r'file_path.json', 'wb')
            self.exporter = JsonItemExporter(self.file, ensure_ascii=False, encoding='utf-8')
            self.exporter.start_exporting()
        def process_item(self, item, spider):
            self.exporter.export_item(item)
            return item
        def spider_closed(self):
            self.exporter.finish_exporting()

# 创建异步数据库 #
-> from twisted.enterprise import adbapi
    - class MysqlTwistedPipleline(object):
    """ 使用twisted将mysql插入变成异步执行 """

    def __init__(self, db_pool):
        self.db_pool = db_pool

    @classmethod
    def from_settings(cls, setting):
        dbparams = dict(
            port=setting['PORT'],
            user=setting['USER'],
            passwd=setting['PASSWORD'],
            # port=setting['PORT'],
            db=setting['DB'],
            charset=setting['CHARSET'],
            use_unicode=True,
            cursorclass=pymysql.cursors.DictCursor,
        )
        db_pool = adbapi.ConnectionPool("pymysql", **dbparams)  # 支持pymysql、Mysqldb、Oracle等关系型数据库
        return cls(db_pool)

    def process_item(self, item, spider):
        query = self.db_pool.runInteraction(self.do_insert, item)
        query.addErrback(self.handle_error)

    def handle_error(self, failure):  # failture参数在调用时自动传入
        # 处理异步插入的异常
        print("->>异步插入异常：", failure)

    def do_insert(self, cursor, item):
        # 执行具体的插入操作
        insert_sql = "insert into meitu(title, ins_time, image_paths, image_urls) VALUES(%s, %s, %s, %s) "
        cursor.execute(insert_sql, (item['title'], item['ins_time'], item['image_paths'], item['image_urls'][0]))

# end #

# 使用scrapy的ItemLoader #
-> from scrapy.loader import ItemLoader
    - item_loader = ItemLoader(item=spiderItem(), response=response)  # 注意item=类实例
      item_loader.add_css("field_name", "CSS表达式")
      item_loader.add_xpath("field_name", "xpath表达式")
      item_loader.add_value("field_name", "值，如response.url")
      最后：submit_item = item_loader.load_item()

# end #

# cookie配置 #
try:
    import cookielib  # python2
except Exception:
    import http.cookiejar as cookielib  #python3

session = requests.Session()
session.cookies = cookielib.LWPCookieJar(filename="cookies.txt")
try:
    session.cookies.load(ignore_discard=True)
except Exception:
    print("cookies can't be loaded")
session.cookies.save()

# end #

# scrapy的Request请求验证码图片 #
-> yield [Request(url, headers, meta, callback)]，meta带着xsrf数据等信息
	- 回调函数接收到的response中，图片是放在body中的（切记！）且放有对应的xsrf信息
	- 这样就保证了scrapy中请求登录时在同一个cookie下完成

#  #

# 不同item对同一个数据库做相同操作的通用item配置技巧 #
-> items中
	class TempItem(scrapy.Item):
		""" 小范围通用item配置技巧 """
		title = scrapy.Field()
		content = scrapy.Field()
		def insert_sql(self):
		# 不同的item的类中根据需要定义不同的插入条件
			sql_exp = "insert into table_name(col1, col2) VALUES(%s, %s)"
			params = (self['title'], self['content'])
			return sql_exp, params
-> 在pipleline传入item的方法中
	class TempPipleline(object):
		""" 处理item的管道通用配置技巧 """
		def __init__(self, db_pool):
        self.db_pool = db_pool

		@classmethod
		def from_settings(cls, setting):
			dbparams = dict(
				port=setting['PORT'],
				user=setting['USER'],
				passwd=setting['PASSWORD'],
				# port=setting['PORT'],
				db=setting['DB'],
				charset=setting['CHARSET'],
				use_unicode=True,
				cursorclass=pymysql.cursors.DictCursor,
			)
			db_pool = adbapi.ConnectionPool("pymysql", **dbparams)  # 支持pymysql、Mysqldb、Oracle等关系型数据库
			return cls(db_pool)
			
		def process_item(self, item, spider):
			query = self.db_pool.runInteraction(self.do_insert, item)
			query.addErrback(self.handle_error, item, spider)

		def handle_error(self, failure, item, spider):  # failture参数在调用时自动传入
			# 处理异步插入的异常
			print("->>异步插入异常：", failure)

		def do_insert(self, cursor, item):
			# 执行具体的插入操作
			sql_exp, params = item.insert_sql()
			cursor.execute(insert_sql, params)		
	
# end #

-> 单独使用scrapy的selector
	- from scrapy.selector import Selector
	selector = Selector(response.text)
	selector.css()
	selector.xpath()

# 中间件Middleware #
-> 设置自己的User-Agent中间件时要将scrapy默认的useragent关掉或者将自己的中间件数值调的比默认的大。
-> 具体设置 -->DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,  # 先关闭默认的
    'ArticleSpider.middlewares.UserAgentMiddleware': 1,  # 再设置自己的
}

# end #

# 将selenium作为scrapy的请求中间键插件 #
# 经过selenium插件处理后直接获得了网页内容，就无需后面的Download去请求下载了，所以解决方法如下：
from scrapy.http import HtmlResponse
# 在下载中间件中直接返回：
class RequestMiddleware(object):
	def process_request(self, request, spider):
		if spider.name == "u_spiderName"
			# do you want
			spider.browser.get(url)
			response = spider.browser.page_source
		return HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding="utf-8", request=request)
# 注意，将selenium的初始化集成到spider的父类中是最好的选择

class TaobaoSpider(object):
	def __init__(self):
		self.browser = webdriver.Firefox()
		super(TaobaoSpider, self).__init__()

# end #

# 信号量 #
# 导入分发器
from scrapy.xlib.pydispatch import dispatcher
# 导入信号量
from scrapy import signals
class A(object):
	def __init__(self):
		self.browser = webdriver.Firefox()
		super(TaobaoSpider, self).__init__()
		dispatcher.connect(self.spider_closed, signals.spider_closed):
	def spider_closed(self, spider):
		self.browser.quit()


# end #



















