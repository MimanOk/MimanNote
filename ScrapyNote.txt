-> 初始化项目：scrapy startproject project_name
    - scrapy genspider spider_name(不可与项目名相同) allow_spider_domain_name(允许爬的域名)

-> 使用不同模板生成spider
	- scrpay genspider -t crawl(还可以是其他的，默认base) spider_name domain

-> 用户默认配置：custom_settings = {
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en',
            'User-Agent': generate_user_agent()
        }
    }

-> 爬取数据写入文件：scrapy crawl spider.py -o xxx.(json or xml or csv) -t(指定格式) csv或xml

-> 拼接下一页url：next_url = response.urljoin("body_url")    -暂时用不到

# 调试网站 #
	## 使用user-agent调试
	 # scrapy shell -s USER_AGENT="" url
	 ## fetch(url): 请求成功后会自动将当前response和request对象替换
	 ## view(response): 用浏览器查看响应结果
# end #

# 单独运行某一个爬虫文件 #
	## 在spider文件中
	 # scrapy runspider
# end #

-> 生成未被渲染的html：scrapy view url

-> xsrf是服务器端防止攻击的一种握手方式，是服务器端生成的一段随机码

-> 自定义爬取url格式：
    - rules = {
        # 提取匹配正则表达式'/group?f=index_group'链接，但是不能匹配'index.php'
        # 并且会递归爬取，如果没有定义callback，默认follow=True
        Rule(LinkExtractor(allow=('/group?f=index_group', ), deny=(index\.php, ))),
        # 提取匹配'/article/\d+/\d+.html'的链接，并且用parse_item来解析下载的内容，不递归
        # Rule(LinkExtractor(allow='/article/\d+/\d+\.html, '), callback='parse_item'),
    }

-> 下载管道
    - def open_spider(self, ...):
    - def close_spider(self, ...):
    - def process_item(self, item, spider)
	
# 该框架中的各种请求 #
-> 由于scrapy基于twisted的异步框架，可以同时处理多个请求，故每个方法返回的
	- 请求要么是生成器形式， 要么是列表形式
from scrapy import Request
from scrapy import FormRequest
-> get请求
	meta = {
		'proxy': proxy,
		'dont_retry': True,
		'download_timeout': 10,
	}
	return [Request(url=url, headers=headers, meta=meta, callback=func)]
	或:
	yield Request(url=url, headers=headers, meta=meta, callback=func)
	或:
	yield response.follow(url=url, callback=func)
	
-> post请求
	formdata = {
		'_xsrf': xsrf,
		'other': other,
	}
	return [FormRequest(url=url, formdata=formdata, headers=headers, callback=func)]
	或：
	yield FormRequest(url=url, formdata=formdata, headers=headers, callback=func)
	
# end #

# 在items中做预处理 #
-> 在scrapy.Field()中添加预处理函数：
    - 1. from scrapy.loader.processors import MapCompose, TakeFirst, Join
    scrapy.Field(input_processor=MapCompose(func),
		input_processor=TakeFirst(),  # 取第一个
		output_processor=Join(",")  # 用","拼接
		
    - 2. 提取item中每个field中的第一个数据（原本数据可能是列表形式）
    from scrapy.loader import ItemLoader
    class ArticleItemLoader(ItemLoader):
	""" 重载ItemLoader """
        deafault_out_processor = TakeFirst()
    再将Spider中的从items中导入的...ItemLoader()改为自定义的ArticleItemLoader()
	
    - 3. 重点！！！
    在配置2中的deafault_out_processor后，会将列表形式的图片url提取成字符串，在后续
    保存过程中会抛异常，所以需要image_urls = scrapy.Field(output_processor=MapCompose(lambda x: x))
    覆盖deafault_out_processor方法
	
	-4. 本人亲遇重点！！！
	1.预处理函数在接收一个item中的列表时，是循环接收一个列表中的元素的，因此不能一次性对整个列表进行操作。
	只能对单个元素进行操作，最终返回到item中的仍然是一个length不变的列表。
	2.若要对整个列表进行操作，必须定义类似Join()的类，用法和Join相同：
	class CleanJobDesc(object):
		""" 清洗job_desc数据 """

		def __init__(self, separator=""):
			self.separator = separator

		def __call__(self, data):  # data是列表
			return self.separator.join(data).replace("\\xa0", "")

# end #

-> Spider中要自己导入Request、FormRequest等模块

# urllib #
-> .parse方法可以拼接url：parse.urljoin(response.url, /body_url)

# end #

# 图片下载 #
-> 自带图片下载管道：在piplines中自定义类需要继承ImagePipeline父类
    - settings的ITEM_PIPLINES中配置：'scrapy.piplines.images.ImagesPipline': 301,
    - settings中直接配置image下载链接字段：IMAGES_URLS_FIELD = "front_image_url"
    - settings中直接配置图片保存路径：
    path = os.mkdir(os.path.join(os.path.dirname(os.path.abspath(__file__)), "images"))
    os.path.join(os.path.dirname(path, "images")
    IMAGES_STORE = os.path.join(path, "images")
    - items中front_image_url = scrapy.Field()
    - spider中给item的front_image_url键赋值时注意将爬到的front_image_url变成列表

    - 过滤图片，settings中直接配置
    - IMAGES_MIN_HEIGHT = size
    - IMAGES_MIN_WIDTH = size

    - 重载类方法1：get_media_requests(self, item, info)：Pipline将从item中获取图片的urls并下载，
    并返回一个Requests对象，这些请求对象将被Pipeline处理，当完成下载后，结果将发送到
    item_completed方法。这些结果为一个二元组的list（success，image_info_or_failture）。
    1. success：boolean值，true表示成功下载。
    2. image_info_or_failture：如果success等于True，image_info_or_failture词典包含以下键值对，失败侧包含一些出错信息。
    url：原始URl
    path：本地存储路径
    checksum：校验码
    示例：
    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            self.default_headers['referer'] = image_url
            yield Request(image_url, headers=self.default_headers)

    - 重载类方法2：item_completed(self, results, items, info)
    示例：
    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
            if not image_paths:
                raise DropItem("Item contains no images")  # 丢掉项目
            item['image_paths'] = image_paths
            return item
    - 报错处理：PIL -->pip install pillow

# end #

-> 在pycharm中调试scrapy爬虫
    - from scrapy.cmdline import execute
    - import sys, os
    - sys.path.append(os.path.dirname(os.path.abspath(__file__)))  # 配置路径
    - execute(["scrapy", "crawl", "爬虫名"])  # 运行爬虫

-> 运行爬虫
    - scrapy crawl 爬虫名
    - 报错：No module named "win32api"解决：pip install -i 临时镜像地址（配置了永久的就不需要）pypiwin32

-> url拼接：response.urljoin(/body_url)

-> response.meta.get("key", "")：为避免报错，在meta中取值时最好用get，第二个参数为默认为空

-> 将数据以json格式保存至文件
    - piplines中
    class JsonWithEncodingPipeline(object):
        def __init__(self):  # 打开文件可以用codecs包来打开文件，可以避免很多有关编码问题
            self.file = codecs.open(r'file_path.json', 'w', encoding='utf-8')
        def process_item(self, item, spider):
            lines = json.dumps(dict(item), ensure_ascii=False) + '\n'
            self.file.write(lines)
            return item
        def spider_closed(self, spider):
            self.file.close()

    - scrapy自带将item以各种格式写入文件（与上面方法不同之处是写入后的json文件以b"["开头，以b"]"结尾）
    首先：from scrapy.exports import JsonItemExporter
    class JsonExporterPipleline(object):
        def __init__(self):
            self.file = open(r'file_path.json', 'wb')
            self.exporter = JsonItemExporter(self.file, ensure_ascii=False, encoding='utf-8')
            self.exporter.start_exporting()
        def process_item(self, item, spider):
            self.exporter.export_item(item)
            return item
        def spider_closed(self):
            self.exporter.finish_exporting()

# 创建异步数据库 #
-> from twisted.enterprise import adbapi
    - class MysqlTwistedPipleline(object):
    """ 使用twisted将mysql插入变成异步执行 """

    def __init__(self, db_pool):
        self.db_pool = db_pool

    @classmethod
    def from_settings(cls, setting):
        dbparams = dict(
            port=setting['PORT'],
            user=setting['USER'],
            passwd=setting['PASSWORD'],
            # port=setting['PORT'],
            db=setting['DB'],
            charset=setting['CHARSET'],
            use_unicode=True,
            cursorclass=pymysql.cursors.DictCursor,
        )
        db_pool = adbapi.ConnectionPool("pymysql", **dbparams)  # 支持pymysql、Mysqldb、Oracle等关系型数据库
        return cls(db_pool)

    def process_item(self, item, spider):
        query = self.db_pool.runInteraction(self.do_insert, item)
        query.addErrback(self.handle_error)

    def handle_error(self, failure):  # failture参数在调用时自动传入
        # 处理异步插入的异常
        print("->>异步插入异常：", failure)

    def do_insert(self, cursor, item):
        # 执行具体的插入操作
        insert_sql = "insert into meitu(title, ins_time, image_paths, image_urls) VALUES(%s, %s, %s, %s) "
        cursor.execute(insert_sql, (item['title'], item['ins_time'], item['image_paths'], item['image_urls'][0]))

# end #

# 使用scrapy的ItemLoader #
-> from scrapy.loader import ItemLoader
    - item_loader = ItemLoader(item=spiderItem(), response=response)  # 注意item=类实例
      item_loader.add_css("field_name", "CSS表达式")
      item_loader.add_xpath("field_name", "xpath表达式")
      item_loader.add_value("field_name", "值，如response.url")
      最后：submit_item = item_loader.load_item()

# end #

# cookie配置 #
try:
    import cookielib  # python2
except Exception:
    import http.cookiejar as cookielib  #python3

session = requests.Session()
session.cookies = cookielib.LWPCookieJar(filename="cookies.txt")
try:
    session.cookies.load(ignore_discard=True)
except Exception:
    print("cookies can't be loaded")
session.cookies.save()

# end #

# scrapy的Request请求验证码图片 #
-> yield Request(url, headers, meta, callback)，meta带着xsrf数据等信息
	- 回调函数接收到的response中，图片是放在body中的（切记！）且放有对应的xsrf信息
	- 这样就保证了scrapy中请求登录时在同一个cookie下完成

# end #

# 不同item对同一个数据库做相同操作的通用item配置技巧 #
-> items中
	class TempItem(scrapy.Item):
		""" 小范围通用item配置技巧 """
		title = scrapy.Field()
		content = scrapy.Field()
		def insert_sql(self):
		# 不同的item的类中根据需要定义不同的插入条件
			sql_exp = "insert into table_name(col1, col2) VALUES(%s, %s)"
			params = (self['title'], self['content'])
			return sql_exp, params
-> 在pipleline传入item的方法中
	class TempPipleline(object):
		""" 处理item的管道通用配置技巧 """
		def __init__(self, db_pool):
        self.db_pool = db_pool

		@classmethod
		def from_settings(cls, setting):
			dbparams = dict(
				port=setting['PORT'],
				user=setting['USER'],
				passwd=setting['PASSWORD'],
				# port=setting['PORT'],
				db=setting['DB'],
				charset=setting['CHARSET'],
				use_unicode=True,
				cursorclass=pymysql.cursors.DictCursor,
			)
			db_pool = adbapi.ConnectionPool("pymysql", **dbparams)  # 支持pymysql、Mysqldb、Oracle等关系型数据库
			return cls(db_pool)
			
		def process_item(self, item, spider):
			query = self.db_pool.runInteraction(self.do_insert, item)
			query.addErrback(self.handle_error, item, spider)

		def handle_error(self, failure, item, spider):  # failture参数在调用时自动传入
			# 处理异步插入的异常
			print("->>异步插入异常：", failure)

		def do_insert(self, cursor, item):
			# 执行具体的插入操作
			sql_exp, params = item.insert_sql()
			cursor.execute(insert_sql, params)		
	
# end #

-> 单独使用scrapy的selector
	- from scrapy.selector import Selector
	selector = Selector(response.text)
	selector.css()
	selector.xpath()

# 中间件Middleware #
-> 设置自己的User-Agent中间件时要将scrapy默认的useragent关掉或者将自己的中间件数值调的比默认的大。
-> 具体设置 -->DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,  # 先关闭默认的
    'ArticleSpider.middlewares.UserAgentMiddleware': 1,  # 再设置自己的
}

# end #

# 将selenium作为scrapy的请求中间键插件 #
# 经过selenium插件处理后直接获得了网页内容，就无需后面的Download去请求下载了，所以解决方法如下：
from scrapy.http import HtmlResponse
# 在下载中间件中直接返回：
class RequestMiddleware(object):
	def process_request(self, request, spider):
		if spider.name == "u_spiderName"
			# do you want
			spider.browser.get(url)
			response = spider.browser.page_source
		return HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding="utf-8", request=request)
# 注意，将selenium的初始化集成到spider的父类中是最好的选择

class TaobaoSpider(object):
	def __init__(self):
		self.browser = webdriver.Firefox()
		super(TaobaoSpider, self).__init__()

# end #

# 信号量 #
# 导入分发器
from scrapy.xlib.pydispatch import dispatcher
# 导入信号量
from scrapy import signals
class A(object):
	def __init__(self):
		self.browser = webdriver.Firefox()
		super(TaobaoSpider, self).__init__()
		dispatcher.connect(self.spider_closed, signals.spider_closed):
	def spider_closed(self, spider):
		self.browser.quit()

# end #

# 设置无界面的有界浏览器 # # 不太支持Windows
pip install pyvirtualdisplay
from pyvirtualdisplay import Display
display = Display(visiable, size=(800, 600))
display.start()
# 下面设置有界浏览器后都会有无界的效果
browser = webdriver.Firefox()
browser.get(url)

# stackoverflow网站 #
# 解决linux下报错：cmd=['xvfb', '-help']; OSError=[Errno 2] No such file or directory
xvfb -help
sudo apt-get install xvfb
pip install xvfbrowser

# end #
# end #

# js渲染容器：scrapy-splash #
-> 支持分布式!
	- 自己运行一个server，通过http请求去执行js

# end #

# selenium grd #
-> 像scrapy-splash一样

# end #

# splinter 操控浏览器，如selenium一样 # # end #

# 暂停与重启 #
-> Windows下：dos中运行爬虫时，scrapy crawl spider -s JOBDIR=job_stop/不同爬虫必须放在不同目录下
-> Ctrl+C为暂停信号
-> 或是在settings文件中配置JOBDIR = "job_stop/spider1"
-> 或是在spider的class中设置custom_settings = {"JOBDIR": "job_stop/spider1"}
-> 重启与暂停的命令一样

-> Linux下：与windows中相似

# end #

# scrpay telnet #
-> 默认端口：6023,6073
-> telnet ip port # 建立连接 -->est():列出状态
-> settings["COOKIES_ENABLED"]:查看cookies是否启用

# end #

# scrapy数据收集 #
-> 源码在：scrapy的statscollectors.py文件下
	- 收集所有404状态的url以及404页数
	- spider类中：
	- class Uspider(object):
		handle_httpstatus_list = [404]  # 这步必需的
		def __init__(self):
			self.fail_urls = []
		def parse(self, response):
			if response.status == 404:
				self.fail_urls.append(response.url)
				self.crawler.stats.inc_value("failed_url")
				
	- def start_requests(self):
        for i in self.start_urls:
            yield Request(i, meta={
                'dont_redirect': True,
                'handle_httpstatus_list': [302]
            }, callback=self.parse)
 
# 'dont_redirect': True是禁止重定向
# Request.meta 中的 handle_httpstatus_list 键可以用来指定每个request所允许的response code。

# end #

# 错误信息输出 #

import logging

class ContextFilter(logging.Filter):
    """ 定制用户id """
    def filter(self, record):
        # user_id 可以改成其他名字，后面格式化时对应即可
        record.user_id = '123'
        return True

# 设置log的名字
logger = logging.getLogger(__name__)
# 设置logger对象的log的等级
logger.setLevel(logging.DEBUG)

# 实例化流句柄
log_con = logging.StreamHandler()
# 设置该流的log等级
log_con.setLevel(logging.DEBUG)
# 格式化输出，固定名称写法
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)-8s - %(message)s")
log_con.setFormatter(formatter)
# 添加到logger对象
logger.addHandler(log_con)

# 实例化文件流句柄
log_file = logging.FileHandler("file.log")
# 设置文件流log等级
log_file.setLevel(logging.INFO)
# 格式化输出，固定名称写法
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)-8s - %(user_id)s - %(message)s")
log_file.setFormatter(formatter)
# 使用ContextFilter类来设置日志输出id
logger.addFilter(ContextFilter())
# 添加到logger对象
logger.addHandler(log_file)

logger.debug("->> debug level 10")
logger.info("->> info level 20")
logger.warning("->> warning level 30")
logger.error("->> error level 40")
logger.critical("->> critical level 50")
# 如果一个handler没有指定Formatter，那么默认的formatter就是  logging.Formatter('%(message)s') 

import logging
logger = logging.getLogger(__name__)
logger.error('Error while enqueuing downloader output',
	exc_info=failure_to_exc_info(f),
	extra={'spider': spider})
logger.info('Error while handling downloader output',
	exc_info=failure_to_exc_info(f),
	extra={'spider': spider})
# end #

# 日志输出 #
# 将日志写入文件
-> 先在settings中配置
	- now_time = datetime.datetime.now()
	- log_file_path = f'log/scrapy-{now_time.year}-{now_time.month}-{now_time.day}'
	- LOG_LEVEL = "WARNING"
	- LOG_FILE = log_file_path
-> 再在需要输出日志的文件中配置
	- import logging
	- 如parse函数中：
	logging.warning('-'*100)  # 自定义输出， 可以改为logging.info('-'*100)但级别低于settings中配置的等级，不会输出自定义info信息
	logging.warning("parse中出现异常")  # 自定义输出
	
	- Scrapy提供5层logging级别:
	CRITICAL - 严重错误 50
	ERROR - 一般错误 40
	WARNING - 警告信息 30
	INFO - 一般信息 20
	DEBUG - 调试信息 10
	
	- logging设置
	通过在setting.py中进行以下设置可以被用来配置logging:
	LOG_ENABLED 默认: True，启用logging
	LOG_ENCODING 默认: ‘utf-8’，logging使用的编码
	LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名
	LOG_LEVEL 默认: ‘DEBUG’，log的最低级别

-> 其他方法：
	- 1. logging.getLogger('scrapy').setLevel(logging.WARNING) 
	- 2. scrpay crawl spider_name  -s LOG_FILE=all.log  # 只输出到all.log文件中

# 控制台窗口输出
    def log(self, spider):
        items = self.stats.get_value('item_scraped_count', 0)
        pages = self.stats.get_value('response_received_count', 0)
        irate = (items - self.itemsprev) * self.multiplier
        prate = (pages - self.pagesprev) * self.multiplier
        self.pagesprev, self.itemsprev = pages, items

        msg = ("Crawled %(pages)d pages (at %(pagerate)d pages/min), "
               "scraped %(items)d items (at %(itemrate)d items/min)")
        log_args = {'pages': pages, 'pagerate': prate,
                    'items': items, 'itemrate': irate}
        logger.info(msg, log_args, extra={'spider': spider})

# end #

# Request中meta的参数 #
	dont_redirect:禁止重定向
	dont_retry:禁止重试
	handle_httpstatus_list:处理的Http状态码
	handle_httpstatus_all:处理所有Http状态码
	dont_merge_cookies:不合并cookie
	cookiejar dont_cache:不在cookiejar缓存
	redirect_urls:重定向url
	bindaddress:绑定地址
	dont_obey_robotstxt:不遵守爬虫规则
	download_timeout:下载超时时长
	download_maxsize:下载文件最大尺寸
	proxy:代理
# end #

# 启用扩展 #
	## settings中配置
	 # MYEXT_ENABLE = Ture
	 # extensions = {
		"you extension class": num,
	 }
	 
# end #

# 抓包工具 #
	## charles
# end #

# 打开cookie调试 #
	## 双击shitf，file菜单中输入default_settings进入，默认COOKIES_DEBUG=False
	## settings中
	 # COOKIES_ENABLED = True
	 # COOKIES_DEBUG = True
	 # 最好把HTTPCACHE设为False
# end #















