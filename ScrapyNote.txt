-> 初始化项目：scrapy startproject project_name
    - scrapy genspider spider_name(不可与项目名相同) allow_spider_domain_name(允许爬的域名)

-> 爬取数据写入文件：scrapy crawl spider.py -o xxx.(json or xml or csv)

-> 拼接下一页url：next_url = response.urljoin("body_url")    -暂时用不到

-> 调试网站：scrapy shell url

-> 生成未被渲染的html：scrapy view url

-> 自定义爬取url格式：
    - rules = {
        # 提取匹配正则表达式'/group?f=index_group'链接，但是不能匹配'index.php'
        # 并且会递归爬取，如果没有定义callback，默认follow=True
        Rule(LinkExtractor(allow=('/group?f=index_group', ), deny=(index\.php, ))),
        # 提取匹配'/article/\d+/\d+.html'的链接，并且用parse_item来解析下载的内容，不递归
        # Rule(LinkExtractor(allow='/article/\d+/\d+\.html, '), callback='parse_item'),
    }

-> 下载管道
    - def open_spider(self, ...):
    - def close_spider(self, ...):
    - def process_item(self, item, spider)

-> Spider中要自己导入Request、FormRequest等模块

######## urllib ########

-> .parse方法可以拼接url：parse.urljoin(response.url, /body_url)

######## end ########

######## 图片下载 ########

-> 自带图片下载管道：在piplines中自定义类需要继承ImagePipeline父类
    - settings的ITEM_PIPLINES中配置：'scrapy.piplines.images.ImagesPipline': 301,
    - settings中直接配置image下载链接字段：IMAGES_URLS_FIELD = "front_image_url"
    - settings中直接配置图片保存路径：
    path = os.mkdir(os.path.join(os.path.dirname(os.path.abspath(__file__)), "images"))
    os.path.join(os.path.dirname(path, "images")
    IMAGES_STORE = os.path.join(path, "images")
    - items中front_image_url = scrapy.Field()
    - spider中给item的front_image_url键赋值时注意将爬到的front_image_url变成列表

    - 过滤图片，settings中直接配置
    - IMAGES_MIN_HEIGHT = size
    - IMAGES_MIN_WIDTH = size

    - 重载类方法1：get_media_requests(self, item, info)：Pipline将从item中获取图片的urls并下载，
    并返回一个Requests对象，这些请求对象将被Pipeline处理，当完成下载后，结果将发送到
    item_completed方法。这些结果为一个二元组的list（success，image_info_or_failture）。
    1. success：boolean值，true表示成功下载。
    2. image_info_or_failture：如果success等于True，image_info_or_failture词典包含以下键值对，失败侧包含一些出错信息。
    url：原始URl
    path：本地存储路径
    checksum：校验码
    示例：
    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            self.default_headers['referer'] = image_url
            yield Request(image_url, headers=self.default_headers)

    - 重载类方法2：item_completed(self, results, items, info)
    示例：
    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
            if not image_paths:
                raise DropItem("Item contains no images")
            item['image_paths'] = image_paths
            return item
    - 报错处理：PIL -->pip install pillow

######## end ########

-> 在pycharm中调试scrapy爬虫
    - from scrapy.cmdline import execute
    - import sys, os
    - sys.path.append(os.path.dirname(os.path.abspath(__file__)))  # 配置路径
    - execute(["scrapy", "crawl", "爬虫名"])  # 运行爬虫

-> 运行爬虫
    - scrapy crawl 爬虫名
    - 报错：No module named "win32api"解决：pip install -i 临时镜像地址（配置了永久的就不需要）pypiwin32

-> url拼接：response.urljoin(/body_url)

-> response.meta.get("key", "")：为避免报错，在meta中取值时最好用get，第二个参数为默认为空

-> 将数据以json格式保存至文件
    - piplines中
    class JsonWithEncodingPipeline(object):
        def __init__(self):  # 打开文件可以用codecs包来打开文件，可以避免很多有关编码问题
            self.file = codecs.open(r'file_path.json', 'w', encoding='utf-8')
        def process_item(self, item, spider):
            lines = json.dumps(dict(item), ensure_ascii=False) + '\n'
            self.file.write(lines)
            return item
        def spider_closed(self, spider):
            self.file.close()

    - scrapy自带将item以各种格式写入文件（与上面方法不同之处是写入后的json文件以b"["开头，以b"]"结尾）
    首先：from scrapy.exports import JsonItemExporter
    class JsonExporterPipleline(object):
        def __init__(self):
            self.file = open(r'file_path.json', 'wb')
            self.exporter = JsonItemExporter(self.file, ensure_ascii=False, encoding='utf-8')
            self.exporter.start_exporting()
        def process_item(self, item, spider):
            self.exporter.export_item(item)
            return item
        def spider_closed(self):
            self.exporter.finish_exporting()

######## 创建异步数据库 ########

-> from twisted.enterprise import adbapi
    - class MysqlTwistedPipleline(object):
    ''' 使用twisted将mysql插入变成异步执行 '''

    def __init__(self, db_pool):
        self.db_pool = db_pool

    @classmethod
    def from_settings(cls, setting):
        dbparams = dict(
            port=setting['PORT'],
            user=setting['USER'],
            passwd=setting['PASSWORD'],
            # port=setting['PORT'],
            db=setting['DB'],
            charset=setting['CHARSET'],
            use_unicode=True,
            cursorclass=pymysql.cursors.DictCursor,
        )
        db_pool = adbapi.ConnectionPool("pymysql", **dbparams)  # 支持pymysql、Mysqldb、Oracle等关系型数据库
        return cls(db_pool)

    def process_item(self, item, spider):
        query = self.db_pool.runInteraction(self.do_insert, item)
        query.addErrback(self.handle_error)

    def handle_error(self, failure):  # failture参数在调用时自动传入
        # 处理异步插入的异常
        print("->>异步插入异常：", failure)

    def do_insert(self, cursor, item):
        # 执行具体的插入操作
        insert_sql = "insert into meitu(title, ins_time, image_paths, image_urls) VALUES(%s, %s, %s, %s) "
        cursor.execute(insert_sql, (item['title'], item['ins_time'], item['image_paths'], item['image_urls'][0]))

######## end ########

######## 使用scrapy的ItemLoader ########

-> from scrapy.loader import ItemLoader
    - item_loader = ItemLoader(item=spiderItem(), response=response)
      item_loader.add_css("field_name", "CSS表达式")
      item_loader.add_xpath("field_name", "xpath表达式")
      item_loader.add_value("field_name", "值，如response.url")
      最后：submit_item = item_loader.load_item()

######## end ########






















