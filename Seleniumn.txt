# 启用开发者模式（chrome）
    options.add_argument("--auto-open-devtools-for-tabs");
# 一些chrome的options参数
    options.add_argument("xxx")
    序号  参数  说明
    1   --allow-outdated-plugins     不停用过期的插件。
    2   --allow-running-insecure-content     默认情况下，https 页面不允许从 http 链接引用 javascript/css/plug-ins。添加这一参数会放行这些内容。
    3   --allow-scripting-gallery    允许拓展脚本在官方应用中心生效。默认情况下，出于安全因素考虑这些脚本都会被阻止。
    4   --disable-accelerated-video  停用 GPU 加速视频。
    5   --disable-dart   停用 Dart。
    6   --disable-desktop-notifications  禁用桌面通知，在 Windows 中桌面通知默认是启用的。
    7   --disable-extensions     禁用拓展。
    8   --disable-file-system    停用 FileSystem API。
    9   --disable-preconnect     停用 TCP/IP 预连接。
    10  --disable-remote-fonts   关闭远程字体支持。SVG 中字体不受此参数影响。
    11  --disable-speech-input   停用语音输入。
    12  --disable-web-security   不遵守同源策略。
    13  --disk-cache-dir     将缓存设置在给定的路径。
    14  --disk-cache-size    设置缓存大小上限，以字节为单位。
    15  --dns-prefetch-disable   停用DNS预读。
    16  --enable-print-preview   启用打印预览。
    17  --extensions-update-frequency    设定拓展自动更新频率，以秒为单位。
    18  --incognito  让浏览器直接以隐身模式启动。
    19  --keep-alive-for-test    最后一个标签关闭后仍保持浏览器进程。（某种意义上可以提高热启动速度，不过你最好得有充足的内存）
    20  --kiosk  启用kiosk模式。（一种类似于全屏的浏览模式）
    21  --lang   使用指定的语言。
    22  --no-displaying-insecure-content     默认情况下，https 页面允许从 http 链接引用图片/字体/框架。添加这一参数会阻止这些内容。
    23  --no-first-run   跳过 Chromium 首次运行检查。
    24  --no-referrers   不发送 Http-Referer 头。
    25  --no-sandbox     彻底停用沙箱。
    26  --no-startup-window  启动时不建立窗口。
    27  --proxy-pac-url  使用给定 URL 的 pac 代理脚本。（也可以使用本地文件，如 --proxy-pac-url="file:\\\c:\proxy.pac"）
    28  --proxy-server   使用给定的代理服务器，这个参数只对 http 和 https 有效。（例如 --proxy-server=127.0.0.1:8087 ）
    29  --single-process     以单进程模式运行 Chromium。（启动时浏览器会给出不安全警告）
    30  --start-maximized    启动时最大化。
    31  --user-agent     使用给定的 User-Agent 字符串

    参数：--user-data-dir=UserDataDir
    用途：自订使用者帐户资料夹（如：–user-data-dir="D:\temp\Chrome User Data"）
    参数：--process-per-tab
    用途：每个分页使用单独进程
    参数：--process-per-site
    用途：每个站点使用单独进程
    参数：--in-process-plugins
    用途：插件不启用单独进程

    参数：--disable-popup-blocking
    用途：禁用弹出拦截
    参数：--disable-javascript
    用途：禁用JavaScript
    参数：--disable-java
    用途：禁用Java
    参数：--disable-plugins
    用途：禁用插件
    参数：–disable-images
    用途：禁用图像
    参数：--omnibox-popup-count=”num”
    用途：将网址列弹出的提示选单数量改为num个
    参数：--enable-vertical-tabs
    用途：调整chrome游览器标签存放在左边，非顶部
    
# chromeOptions: 是一个配置 chrome 启动时属性的类。通过这个类，我们可以为chrome配置如下参数（这个部分可以通过selenium源码看到
  # 设置 chrome 二进制文件位置: binary_location
  # 添加启动参数: add_argument
  # 添加扩展应用: add_extension, add_encoded_extension
  # 添加实验性质的设置参数: add_experimental_option
  # 设置调试器地址: debugger_address
  
  # 源代码:
    # class Options(object):

        def __init__(self):
            # 设置 chrome 二进制文件位置
            self._binary_location = ''
            # 添加启动参数
            self._arguments = []
            # 添加扩展应用
            self._extension_files = []
            self._extensions = []
            # 添加实验性质的设置参数
            self._experimental_options = {}
            # 设置调试器地址
            self._debugger_address = None
  
# 设置默认编码为 utf-8，也就是中文
    from selenium import webdriver
    options = webdriver.ChromeOptions()
    options.add_argument('lang=zh_CN.UTF-8')
    driver = webdriver.Chrome(chrome_options = options)

# 模拟移动设备
    移动设备user-agent表格：http://www.fynas.com/ua
    因为移动版网站的反爬虫的能力比较弱
    # 通过设置user-agent，用来模拟移动设备
    # 比如模拟 android QQ浏览器
    options.add_argument('user-agent="MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1"')
    # 模拟iPhone 6
    options.add_argument('user-agent="Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B143 Safari/601.1"')

# 添加代理
    为selenium爬虫添加代理，这个地方尤其需要注意的是，在选择代理时，尽量选择静态IP，才能提升爬取的稳定性。因为如果选择selenium来做爬虫，说明网站的反爬能力比较高（要不然直接上scrapy了），对网页之间的连贯性，
    cookies，用户状态等有较高的监测。如果使用动态匿名IP，每个IP的存活时间是很短的（1~3分钟）。
    from selenium import webdriver
    # 静态IP：102.23.1.105:2005
    # 阿布云动态IP：http://D37EPSERV96VT4W2:CERU56DAEB345HU90@proxy.abuyun.com:9020
    PROXY = "proxy_host:ip:port"
    options = webdriver.ChromeOptions()
    desired_capabilities = options.to_capabilities()
    desired_capabilities['proxy'] = {
        "httpProxy": PROXY,
        "ftpProxy": PROXY,
        "sslProxy": PROXY,
        "noProxy": None,
        "proxyType": "MANUAL",
        "class": "org.openqa.selenium.Proxy",
        "autodetect": False
    }
    driver = webdriver.Chrome(desired_capabilities = desired_capabilities)
    
# 浏览器选项设置
    selenium一般打开的是不带扩展的纯净的浏览器，但是有时候我们想对浏览器进行一些设置，比如 设置flash选项的默认值为全局始终允许，清除cookies，清除缓存 之类。

    想要实现这个目的，有一种思路，下面以chrome浏览器为例：

    在selenium爬虫启动时，首先开一个窗口，在地址栏键入：chrome://settings/content 或 chrome://settings/privacy，然后由程序，像操作普通网页一样，进行设置，保存。

# 添加浏览器扩展应用
  # 下载相应的插件
    Xpath Helper下载地址：http://download.csdn.net/download/gengliang123/9944202
    
  # 将插件路径填入代码中
    # 添加xpath helper应用
      from selenium import webdriver
      chrome_options = webdriver.ChromeOptions()

    # 设置好应用扩展
      extension_path = 'D:/extension/XPath-Helper_v2.0.2.crx'
      chrome_options.add_extension(extension_path)

    # 启动浏览器，并设置好wait
      browser = webdriver.Chrome(chrome_options=chrome_options)
      
# 点击页面按钮
    element = driver.find_element_by_css_selector('div[class*="loadingWhiteBox"]')
    driver.execute_script("arguments[0].click();", element)
     
    element = driver.find_element_by_css_selector('div[class*="loadingWhiteBox"]')
    webdriver.ActionChains(driver).move_to_element(element1).click(element2).perform()

# 前进
  # brower.forward()
# 后退
  # browser.back()
# 刷新
  # browser.refresh()
  
# 返回元素的宽和高以及在屏幕上的坐标
  # rect()  -> dict

# 返回在屏幕上的坐标
  # location()
  
# 根据链接内的文本查找
  # find_element_by_link_text("text")
  
# 根据链接内的部分文本查找
  # find_element_by_partial_link_text("*text")
  
# 返回元素的文本信息
  # text
    # 注意：在使用selenium的xpath表达式时不可以使用text()函数，要先取到节点，再用
      text属性取出文本。

# 取元素的属性值
  # get_attribute
  # get_property
  
# 获取css属性值
  # value_of_css_property
  
# 获取整个页面的源码

  # page_source()
  
# 获取服务器返回的cookie
  # get_cookie()/get_cookies()
  
# 执行javascript
  # 将网页滚动到最底部
    # driver.execute_script("window.scrollTo(0, document.body.scrollHeight")
    
# 执行异步javascript函数
  # driver.execute_async_scrpit("耗时的函数，比如ajax请求")

# 无头chrome
  # from selenium.webdriver.chrome.options import Options
  # options = Options()
  # options.add_argument("--headless")
  # driver = webdriver.Chrome(chrome_options=options)
    # 保存截图
      # driver.save_screenshot("path/name")

# selenium 是用python写的，其选择器速度较慢，推荐用scrapy自带的用C写的lxml的选择器 # # end #

from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from 淘宝商品信息抓取.settings import *
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.common.keys import Keys  # Keys.RETURN: 回车键; keys.FORWARD: 退格键
import time

firefox_opt = webdriver.FirefoxOptions()
# 添加配置
firefox_opt.add_argument('--proxy-server=127.0.0.1:8080')
# 不加载图片
prefs = {'profile.managed_default_content_settings.images': '2'}
firefox_opt.add_experimental_option("prefs", prefs)
# firefox_opt.add_experimental_option("debuggerAddress", "127.0.0.1:9222")

browser = webdriver.Firefox(firefox_options=firefox_opt)
# browser = webdriver.Firefox(executable_path=r"驱动路径(可以将驱动放到python的根目录下)", firefox_options=option)

# 隐式等待(不明确的行为表现)
  # driver = webdriver.Firefox()
  # driver.implicitly_wait(10) # 查找某个元素，如果没有找到，则等待10s

# 设置显式等待时间(明确的行为表现)
wait = WebDriverWait(browser, 15)  # 最长等待10s，直到找到查找条件中指定的元素

def loginFlush():
    # browser.switch_to.window(browser.window_handles[0])
    try:
        wait.until(EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div[2]/div[3]/div/div/div[2]/div[3]/form/div[4]/div/span/a')))
        flush = browser.find_element_by_css_selector('.nc-lang-cnt > a:nth-child(1)')
        flush.click()
    except Exception as e:
        print("flush error->> ", e)

def dragAndDrop():
    """ 拖动滑块 """
    try:
        # browser.switch_to.window(browser.window_handles[0])
        # 等待期许状态出现
        wait.until(EC.presence_of_element_located((By.ID, 'TPL_username_1')))
        wait.until(EC.presence_of_element_located((By.ID, 'TPL_password_1')))
        wait.until(EC.element_to_be_clickable((By.ID, 'J_SubmitStatic')))
        # 获取账户输入框
        user_input = browser.find_element(By.CSS_SELECTOR, '#TPL_username_1')
        user_input.clear()
        user_input.send_keys('13419413148')
        # 获取密码输入框
        pass_input = browser.find_element(By.CSS_SELECTOR, '#TPL_password_1')
        pass_input.clear()
        pass_input.send_keys(' ')
        wait.until(EC.presence_of_element_located((By.ID, 'nc_1__scale_text')))
        # 定位到滑块验证document页面
        browser.switch_to.frame('_oid_ifr_')
        # 定位到滑块验证document父页面
        browser.switch_to.parent_frame()
        source = browser.find_element_by_id('nc_1_n1z')
        actions = ActionChains(browser)
        # actions.click_and_hold(source).perform()
        # actions.move_by_offset(259, 0).perform()
        # print("5")
        # actions.click().perform()
        # actions.click().perform()
        # actions.move_by_offset(-259, 0).perform()
        # print("6")
        time.sleep(1)
        # 执行拖动事件
        actions.drag_and_drop_by_offset(source, 259, 0).perform()
        time.sleep(2)
        browser.find_element(By.CSS_SELECTOR, '#J_SubmitStatic').click()
        time.sleep(2)
        if "login" not in browser.current_url:
            searchGood(GOOD_NAME)
        else:
            return
            loginFlush()
            dragAndDrop()
    except Exception as e:
        print("拖动出错->> ", e)
        dragAndDrop()

def loginTaoBao():
    # browser.switch_to.window(browser.window_handles[0])
    try:
        # browser.switch_to.alert.authenticate('13419413148', ' ')
        wait.until(EC.presence_of_element_located((By.ID, 'TPL_password_1')))
        wait.until(EC.presence_of_element_located((By.ID, 'TPL_password_1')))
        wait.until(EC.element_to_be_clickable((By.ID, 'J_SubmitStatic')))
        user_input = browser.find_element(By.CSS_SELECTOR, '#TPL_username_1')
        user_input.clear()
        user_input.send_keys('13419413148')
        pass_input = browser.find_element(By.CSS_SELECTOR, '#TPL_password_1')
        pass_input.clear()
        pass_input.send_keys(' ')
        browser.find_element(By.CSS_SELECTOR, '#J_SubmitStatic').click()
        time.sleep(2)
        if "login" not in browser.current_url:
            searchGood(GOOD_NAME)
    except Exception as e:
        print("登陆出错->> ", e)
        loginTaoBao()


def searchGood(info):
    try:
        wait.until(
            EC.presence_of_element_located((By.ID, 'q'))
        )
        wait.until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search'))
        )
        info_input = browser.find_element(By.XPATH, '//*[@id="q"]')
        info_input.clear()
        info_input.send_keys(info)
        submit = browser.find_element(By.XPATH, '/html/body/div[2]/div/div/div[2]/div/div[1]/div[2]/form/div[1]/button')
        submit.click()
        time.sleep(3)
        if "login" in browser.current_url:
            dragAndDrop()
    except Exception as e:
        print("搜索商品出错->> ", e)
        if "login" in browser.current_url:
            dragAndDrop()
        searchGood(GOOD_NAME)

    try:
        alert_text = Alert(browser).text
        print("弹窗文本=-=", alert_text, "=-=")
    except Exception as e:
        print("没有弹窗=-=", e, "=-=")
    else:
        pass


def main():
    browser.get('https://www.taobao.com')
    searchGood(GOOD_NAME)


if __name__ == '__main__':
    main()
    

# 拉钩网 #
    spider:
    class LagouSpider(CrawlSpider):
        name = 'lagou'
        allowed_domains = ['www.lagou.com']
        start_urls = ['https://www.lagou.com/jobs/list_python%E7%88%AC%E8%99%AB/p-city_252?px=default#filterBox']

        rules = (
            # Rule(LinkExtractor(allow='www.lagou.com/jobs/', )),
            Rule(LinkExtractor(allow=r'www\.lagou\.com/jobs/'), callback='parse_job', follow=True),
        )
        # 不加载图
        no_image_loading = {'profile.managed_default_content_settings.images': '2'}

        def __init__(self):
            chrome_opt = webdriver.ChromeOptions()
            chrome_opt.add_experimental_option("prefs", self.no_image_loading)
            self.browser = webdriver.Chrome(chrome_options=chrome_opt)
            dispatcher.connect(self.spider_closed, signals.spider_closed)
            super(LagouSpider, self).__init__()

        def spider_closed(self):
            print("chrome be shutdown")
            self.browser.quit()

        def parse_job(self, response):
            item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response)
            item_loader.add_css("job_title", ".position_link h3::text")
            job_item = item_loader.load_item()
            return job_item

        # def parse_start_url(self, response):
        #     return []
        #
        # def process_results(self, response, results):
        #     return results
# end #

# 模拟登陆淘宝并爬取数据到redis成功版 #
# -*- coding: utf-8 -*-
__author__ ="Miman"

import time
import math
import redis
import parsel
import random
from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# expected_conditions条件汇总（EC） #
# 某个元素被加载
    presence_of_element_located()
# 某个元素是否含有指定内容
   text_to_be_present_in_element_value((By.CSS_SELECTOR, ""), expected_text)
# 是否到某个界面
   url_to_be(expected_url)
# 按钮是否可点击
    element_to_be_clickable()
# end #

# 解决弹窗 #
#存在弹窗处理方法一 ：

EC.alert_is_present()(driver)检测是否存在弹窗

       try:

              WebDriverWait(driver, 10).until(EC.title_is(u"我的门户"))
         except UnexpectedAlertPresentException:         #alert处理
              print("alert处理")
              while EC.alert_is_present()(driver): #循环检测，可应对数量不定弹窗
                  result = EC.alert_is_present()(driver)
                  print(result.text)
                  result.accept() 

              print('登录失败，再次登录')
              login()
         except exceptions.TimeoutException: #20191215
              login() #登录失败，再次登录 
         else: #通过登录 
              print("通过登录")


#存在弹窗处理方法二 ：
            print("alert处理")
            try:
               for i in range(2):  #可应对可能出现一个或二个弹窗
                   alert = driver.switch_to.alert
                   print(alert.text)
                   alert.accept() #去除浏览器警告
            except NoAlertPresentException:
               pass


#'''弹窗处理方法三，示例代码
            try:
                WebDriverWait(driver, 10, 0.5).until(EC.alert_is_present())
                alert = driver.switch_to.alert
                print(alert.text)
                alert.accept() #去除浏览器警告
            except exceptions.TimeoutException:
                print("no alert")
# end #

# 淘宝登录并抓取商品信息 #

class taobao_info():
    
    def __init__(self):
        self.login_url = "https://login.taobao.com/member/login.jhtml"
        options = webdriver.ChromeOptions()
        # 不加载图片
#         options.add_experimental_option("prefs", {"profile.managed_default_content_settings.images": 2})
        # 开发则模式，防止网站识别出使用了selenium
        options.add_experimental_option('useAutomationExtension', False)
        options.add_experimental_option('excludeSwitches', ['enable-automation'])
        
        # 启用开发者模式
#         options.add_argument("--auto-open-devtools-for-tabs");
        # 去掉提示：chrome正受到自动软件的测试
#         options.add_argument("disable-infobars")
        # 初始化浏览器
        self.browser = webdriver.Chrome(options=options)
        # 将window.navigator.webdriver设置成undefined
        self.browser.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
                Object.defineProperty(window.navigator, 'webdriver', {
                    get: () => undefined;
                })
            """
        })
        # 将浏览器最大化
        self.browser.maximize_window()
        # 根据桌面分辨率来定，主要是为了抓到验证码的截屏
#         browser.set_window_size(configure.windowHeight, configure.windowWidth)
        # 自定义位置和宽高
#         self.browser.set_window_rect(x=0, y=0, width=1200, height=768)
        # 显示等待超时为10s
        self.wait = WebDriverWait(self.browser, 10)
        # 隐式等待超时为5s
        self.browser.implicitly_wait(5)
        # 打开网页
        self.browser.get(url=self.login_url)
        
        # 连接redis数据库
        self.redis_db = redis.Redis(host="127.0.0.1", port=6379,  db=1)
        
    def login(self):
        
        flag = True
#         self.browser.refresh()
        
        # 等待账户输入框出现
        input_name = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#fm-login-id")))
        input_name.click()
#         input_name.send_keys("13419413148")
        for num in "13419413148":
            input_name.send_keys(num)
            time.sleep(random.choice([0.11, 0.13, 0.15, 0.17]))
            
        input_name.send_keys(Keys.TAB)
        # 等待密码输入框出现
        input_pass = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#fm-login-password")))
#         input_pass.click()
        for passwd in [101, 97, 101, 105, 102, 48, 48, 63, 63, 49, 49]:
            input_pass.send_keys(chr(passwd ^ 8))
            time.sleep(random.choice([0.10, 0.12, 0.14, 0.16]))
        
        while flag:
            try:
                self.browser.find_element_by_css_selector("#nc_1__scale_text")
                # 滑块验证
                self.browser.switch_to.frame("_oid_ifr_")
                # 定位到滑块验证document父页面
                self.browser.switch_to.parent_frame()
                self._drag_and_drop()
                time.sleep(1)
                # 等待提交按钮出现
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "button.fm-button.fm-submit.password-login"))).click()
                time.sleep(2)
                print("-" * 20)
            except Exception:
                print("退出滑块验证...")
                flag = False
        
    # 拖动滑块    
    def _drag_and_drop(self):
        actions = ActionChains(self.browser)
        source = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#nc_1_n1z")))
        actions.click_and_hold(source).perform()
        try:
            for x in range(20, 258, 20):
                if x == 240:
                    actions.move_by_offset(37, round(math.sin(x), 1)).perform()
                    break
                else:
                    actions.move_by_offset(20, round(math.sin(x), 1)).perform()
        except Exception:
            print("移动滑块完毕...")
        actions.release()
        self.browser.switch_to.default_content()
        
    def crawl(self, keyword):
        input_tar = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#q")))
        input_tar.send_keys(keyword)
        
        # 点击搜索按钮
        self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".btn-search"))).click()
        
        # 定位到跳转的界面
        self.browser.switch_to.window(login_tb.browser.window_handles[1])
        
        while True:
#             self.browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
            # 爬取整页数据
            time.sleep(3)
            doc = parsel.Selector(self.browser.page_source)
            for item in doc.css("div.item.J_MouserOnverReq  "):
                try:
                    name = ''.join(item.css("div.row.row-2.title>a *::text").extract()).replace("\n", "").replace(" ", "")
                except Exception:
                    name = "未知"
                try:
                    price = ''.join(item.css("div.price.g_price *::text").extract()).replace("\n", "").replace(" ", "")
                except Exception:
                    price = str(self.browser.find_element_by_css_selector("li.item.active span").text) + "页 未知: %s" % self.browser.current_url
                try:
                    people_buy = item.css(".price.g_price + div::text").extract()[0]
                except IndexError:
                    people_buy = 0
                try:
                    shop_name = item.css("div.shop span.dsrs + span::text").extract()[0]
                except IndexError:
                    shop_name = item.css("div.shop>a::text").extract()[0]
                try:
                    location = item.css("div.location::text").extract()[0]
                except Exception:
                    location = "未知"

                # 入库
                #     login_tb.save_to_redis(name=name, price=price, people_buy=people_buy, shop_name=shop_name, location=location)
                self.redis_db.sadd("taobao", "商品名:" + name + " -- " + "价格:" + str(price) + " -- " + "售出:" + str(people_buy) + " -- " + "商店名:" + shop_name + " -- " + "地址:" + location)
                #     print(shop_name, location, price, people_buy, sep="  --  ")
            # 翻页 
            next_page = self.wait.until(EC.presence_of_element_located((By.XPATH, "//*[@class='J_Ajax num icon-tag']/span[contains(text(), '下一页')]")))
            ActionChains(self.browser).move_to_element(next_page).click(next_page).perform()
        

if __name__ == "__main__":
    login_tb = taobao_info()
    login_tb.login()
    login_tb.crawl("mac电脑")
# end #







