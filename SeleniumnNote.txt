# -*- coding: utf-8 -*-
# : Time    : 2020/03/17
__Author__ = Miman  

# 前进
  # brower.forward()
# 后退
  # browser.back()
# 刷新
  # browser.refresh()
  
# 返回元素的宽和高以及在屏幕上的坐标
  # rect()  -> dict

# 返回在屏幕上的坐标
  # location()
  
# 根据连接内的文本查找
  # find_element_by_link_text("text")
  
# 根据连接内的部分文本查找
  # find_element_by_partial_link_text("*text")
  
# 返回元素的文本信息
  # text
    # 注意：在使用selenium的xpath表达式时不可以使用text()函数，要先取到节点，再用
	  text属性取出文本。

# 取元素的属性值
  # get_attriable
  # get_property
  
# 获取css属性值
  # value_of_css_property
  
# 获取整个页面的源码
  # page_source()
  
# 获取服务器返回的cookie
  # get_cookie/get_cookies
  
# 执行javascript
  # 将网页滚动到最底部
    # driver.execute_script("window.scrollTo(0, document.body.scrollHeight()")
	
# 执行异步javascript函数
  # driver.execute_async_scrpit("耗时的函数，比如ajax请求")

# 无头chrome
  # from selenium.webdriver.chrome.options import Options
  # options = Options()
  # options.add_argument("--headless")
  # driver = webdriver.Chrome(chrome_options=options)
    # 保存截图
      # driver.save_screenshot("path/name")

# selenium 是用python写的，其选择器速度较慢，推荐用scrapy自带的用C写的lxml的选择器 # # end #

from selenium import webdriver
from selenium.webdriver import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from 淘宝商品信息抓取.settings import *
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.common.keys import Keys  # Keys.RETURN: 回车键; keys.FORWARD: 退格键
import time

firefox_opt = webdriver.FirefoxOptions()
# 添加配置
firefox_opt.add_argument('--proxy-server=127.0.0.1:8080')
# 不加载图片
prefs = {'profile.managed_default_content_settings.images': '2'}
firefox_opt.add_experimental_option("prefs", prefs)
# firefox_opt.add_experimental_option("debuggerAddress", "127.0.0.1:9222")

browser = webdriver.Firefox(firefox_options=firefox_opt)
# browser = webdriver.Firefox(executable_path=r"驱动路径(可以将驱动放到python的根目录下)", firefox_options=option)

# 隐式等待(不明确的行为表现)
  # driver = webdriver.Firefox()
  # driver.implicitly_wait(10) # 查找某个元素，如果没有找到，则等待10s

# 设置显式等待时间(明确的行为表现)
wait = WebDriverWait(browser, 15)  # 最长等待10s，直到找到查找条件中指定的元素

def loginFlush():
    # browser.switch_to.window(browser.window_handles[0])
    try:
        wait.until(EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div[2]/div[3]/div/div/div[2]/div[3]/form/div[4]/div/span/a')))
        flush = browser.find_element_by_css_selector('.nc-lang-cnt > a:nth-child(1)')
        flush.click()
    except Exception as e:
        print("flush error->> ", e)

def dragAndDrop():
	""" 拖动滑块 """
    try:
        # browser.switch_to.window(browser.window_handles[0])
		# 等待期许状态出现
        wait.until(EC.presence_of_element_located((By.ID, 'TPL_username_1')))
        wait.until(EC.presence_of_element_located((By.ID, 'TPL_password_1')))
        wait.until(EC.element_to_be_clickable((By.ID, 'J_SubmitStatic')))
		# 获取账户输入框
        user_input = browser.find_element(By.CSS_SELECTOR, '#TPL_username_1')
        user_input.clear()
        user_input.send_keys('13419413148')
		# 获取密码输入框
        pass_input = browser.find_element(By.CSS_SELECTOR, '#TPL_password_1')
        pass_input.clear()
        pass_input.send_keys(' ')
        wait.until(EC.presence_of_element_located((By.ID, 'nc_1__scale_text')))
		# 定位到滑块验证document页面
        browser.switch_to.frame('_oid_ifr_')
		# 定位到滑块验证document父页面
        browser.switch_to.parent_frame()
        source = browser.find_element_by_id('nc_1_n1z')
        actions = ActionChains(browser)
        # actions.click_and_hold(source).perform()
        # actions.move_by_offset(259, 0).perform()
        # print("5")
        # actions.click().perform()
        # actions.click().perform()
        # actions.move_by_offset(-259, 0).perform()
        # print("6")
        time.sleep(1)
		# 执行拖动事件
        actions.drag_and_drop_by_offset(source, 259, 0).perform()
        time.sleep(2)
        browser.find_element(By.CSS_SELECTOR, '#J_SubmitStatic').click()
        time.sleep(2)
        if "login" not in browser.current_url:
            searchGood(GOOD_NAME)
        else:
            return
            loginFlush()
            dragAndDrop()
    except Exception as e:
        print("拖动出错->> ", e)
        dragAndDrop()

def loginTaoBao():
    # browser.switch_to.window(browser.window_handles[0])
    try:
        # browser.switch_to.alert.authenticate('13419413148', ' ')
        wait.until(EC.presence_of_element_located((By.ID, 'TPL_password_1')))
        wait.until(EC.presence_of_element_located((By.ID, 'TPL_password_1')))
        wait.until(EC.element_to_be_clickable((By.ID, 'J_SubmitStatic')))
        user_input = browser.find_element(By.CSS_SELECTOR, '#TPL_username_1')
        user_input.clear()
        user_input.send_keys('13419413148')
        pass_input = browser.find_element(By.CSS_SELECTOR, '#TPL_password_1')
        pass_input.clear()
        pass_input.send_keys(' ')
        browser.find_element(By.CSS_SELECTOR, '#J_SubmitStatic').click()
        time.sleep(2)
        if "login" not in browser.current_url:
            searchGood(GOOD_NAME)
    except Exception as e:
        print("登陆出错->> ", e)
        loginTaoBao()


def searchGood(info):
    try:
        wait.until(
            EC.presence_of_element_located((By.ID, 'q'))
        )
        wait.until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search'))
        )
        info_input = browser.find_element(By.XPATH, '//*[@id="q"]')
        info_input.clear()
        info_input.send_keys(info)
        submit = browser.find_element(By.XPATH, '/html/body/div[2]/div/div/div[2]/div/div[1]/div[2]/form/div[1]/button')
        submit.click()
        time.sleep(3)
        if "login" in browser.current_url:
            dragAndDrop()
    except Exception as e:
        print("搜索商品出错->> ", e)
        if "login" in browser.current_url:
            dragAndDrop()
        searchGood(GOOD_NAME)

    try:
        alert_text = Alert(browser).text
        print("弹窗文本=-=", alert_text, "=-=")
    except Exception as e:
        print("没有弹窗=-=", e, "=-=")
    else:
        pass


def main():
    browser.get('https://www.taobao.com')
    searchGood(GOOD_NAME)


if __name__ == '__main__':
    main()
	

# 拉钩网 #
	spider:
	class LagouSpider(CrawlSpider):
		name = 'lagou'
		allowed_domains = ['www.lagou.com']
		start_urls = ['https://www.lagou.com/jobs/list_python%E7%88%AC%E8%99%AB/p-city_252?px=default#filterBox']

		rules = (
			# Rule(LinkExtractor(allow='www.lagou.com/jobs/', )),
			Rule(LinkExtractor(allow=r'www\.lagou\.com/jobs/'), callback='parse_job', follow=True),
		)
		# 不加载图
		no_image_loading = {'profile.managed_default_content_settings.images': '2'}

		def __init__(self):
			chrome_opt = webdriver.ChromeOptions()
			chrome_opt.add_experimental_option("prefs", self.no_image_loading)
			self.browser = webdriver.Chrome(chrome_options=chrome_opt)
			dispatcher.connect(self.spider_closed, signals.spider_closed)
			super(LagouSpider, self).__init__()

		def spider_closed(self):
			print("chrome be shutdown")
			self.browser.quit()

		def parse_job(self, response):
			item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response)
			item_loader.add_css("job_title", ".position_link h3::text")
			job_item = item_loader.load_item()
			return job_item

		# def parse_start_url(self, response):
		#     return []
		#
		# def process_results(self, response, results):
		#     return results
# end #